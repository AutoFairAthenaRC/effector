@book_section{Molnar2020,
   abstract = {We present a brief history of the field of interpretable machine learning (IML), give an overview of state-of-the-art interpretation methods, and discuss challenges. Research in IML has boomed in recent years. As young as the field is, it has over 200 years old roots in regression modeling and rule-based machine learning, starting in the 1960s. Recently, many new IML methods have been proposed, many of them model-agnostic, but also interpretation techniques specific to deep learning and tree-based ensembles. IML methods either directly analyze model components, study sensitivity to input perturbations, or analyze local or global surrogate approximations of the ML model. The field approaches a state of readiness and stability, with many methods not only proposed in research, but also implemented in open-source software. But many important challenges remain for IML, such as dealing with dependent features, causal interpretation, and uncertainty estimation, which need to be resolved for its successful application to scientific problems. A further challenge is a missing rigorous definition of interpretability, which is accepted by the community. To address the challenges and advance the field, we urge to recall our roots of interpretable, data-driven modeling in statistics and (rule-based) ML, but also to consider other areas such as sensitivity analysis, causal inference, and the social sciences.},
   author = {Christoph Molnar and Giuseppe Casalicchio and Bernd Bischl},
   doi = {10.1007/978-3-030-65965-3_28},
   isbn = {9783030659646},
   issn = {18650937},
   journal = {Communications in Computer and Information Science},
   keywords = {Explainable artificial intelligence,Interpretable Machine Learning},
   month = {10},
   pages = {417-431},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Interpretable Machine Learning – A Brief History, State-of-the-Art and Challenges},
   volume = {1323},
   url = {https://link.springer.com/10.1007/978-3-030-65965-3_28},
   year = {2020},
}

@book_section{Molnar2022,
   abstract = {An increasing number of model-agnostic interpretation techniques for machine learning (ML) models such as partial dependence plots (PDP), permutation feature importance (PFI) and Shapley values provide insightful model interpretations, but can lead to wrong conclusions if applied incorrectly. We highlight many general pitfalls of ML model interpretation, such as using interpretation techniques in the wrong context, interpreting models that do not generalize well, ignoring feature dependencies, interactions, uncertainty estimates and issues in high-dimensional settings, or making unjustified causal interpretations, and illustrate them with examples. We focus on pitfalls for global methods that describe the average model behavior, but many pitfalls also apply to local methods that explain individual predictions. Our paper addresses ML practitioners by raising awareness of pitfalls and identifying solutions for correct model interpretation, but also addresses ML researchers by discussing open issues for further research.},
   author = {Christoph Molnar and Gunnar König and Julia Herbinger and Timo Freiesleben and Susanne Dandl and Christian A. Scholbeck and Giuseppe Casalicchio and Moritz Grosse-Wentrup and Bernd Bischl},
   doi = {10.1007/978-3-031-04083-2_4},
   isbn = {9783031040825},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Explainable AI,Interpretable machine learning},
   pages = {39-68},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {General Pitfalls of Model-Agnostic Interpretation Methods for Machine Learning Models},
   volume = {13200 LNAI},
   url = {https://link.springer.com/10.1007/978-3-031-04083-2_4},
   year = {2022},
}


@article{Herbinger2022,
   abstract = {Machine learning models can automatically learn complex relationships, such as non-linear and interaction effects. Interpretable machine learning methods such as partial dependence plots visualize marginal feature effects but may lead to misleading interpretations when feature interactions are present. Hence, employing additional methods that can detect and measure the strength of interactions is paramount to better understand the inner workings of machine learning models. We demonstrate several drawbacks of existing global interaction detection approaches, characterize them theoretically, and evaluate them empirically. Furthermore, we introduce regional effect plots with implicit interaction detection, a novel framework to detect interactions between a feature of interest and other features. The framework also quantifies the strength of interactions and provides interpretable and distinct regions in which feature effects can be interpreted more reliably, as they are less confounded by interactions. We prove the theoretical eligibility of our method and show its applicability on various simulation and real-world examples.},
   author = {Julia Herbinger and Bernd Bischl and Giuseppe Casalicchio},
   month = {2},
   title = {REPID: Regional Effect Plots with implicit Interaction Detection},
   url = {http://arxiv.org/abs/2202.07254},
   year = {2022},
}

@article{Mehrabi2019,
   abstract = {With the widespread use of AI systems and applications in our everyday lives, it is important to take fairness issues into consideration while designing and engineering these types of systems. Such systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that the decisions do not reflect discriminatory behavior toward certain groups or populations. We have recently seen work in machine learning, natural language processing, and deep learning that addresses such challenges in different subdomains. With the commercialization of these systems, researchers are becoming aware of the biases that these applications can contain and have attempted to address them. In this survey we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined in order to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and how they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
   author = {Ninareh Mehrabi and Fred Morstatter and Nripsuta Saxena and Kristina Lerman and Aram Galstyan},
   month = {8},
   title = {A Survey on Bias and Fairness in Machine Learning},
   url = {http://arxiv.org/abs/1908.09635},
   year = {2019},
}

@article{Apley2020,
   abstract = {In many supervised learning applications, understanding and visualizing the effects of the predictor variables on the predicted response is of paramount importance. A shortcoming of black box supervised learning models (e.g. complex trees, neural networks, boosted trees, random forests, nearest neighbours, local kernel-weighted methods and support vector regression) in this regard is their lack of interpretability or transparency. Partial dependence plots, which are the most popular approach for visualizing the effects of the predictors with black box supervised learning models, can produce erroneous results if the predictors are strongly correlated, because they require extrapolation of the response at predictor values that are far outside the multivariate envelope of the training data. As an alternative to partial dependence plots, we present a new visualization approach that we term accumulated local effects plots, which do not require this unreliable extrapolation with correlated predictors. Moreover, accumulated local effects plots are far less computationally expensive than partial dependence plots. We also provide an R package ALEPlot as supplementary material to implement our proposed method.},
   author = {Daniel W. Apley and Jingyu Zhu},
   doi = {10.1111/rssb.12377},
   issn = {14679868},
   issue = {4},
   journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
   keywords = {Functional analysis of variance,Marginal plots,Partial dependence plots,Supervised learning,Visualization},
   note = {The paper that proposed ALE plots.},
   pages = {1059-1086},
   title = {Visualizing the effects of predictor variables in black box supervised learning models},
   volume = {82},
   year = {2020},
}