\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Paper summary}
\author{Vasilis Gkolemis}
\date{July 2021}

\usepackage{graphicx}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[margin=3cm]{geometry}
\usepackage{float}
\graphicspath{ {./../examples/} }

\newcommand{\dfdx}{\frac{\partial f}{\partial x_s}}
\newcommand{\xc}{\mathbf{x_c}}
\newcommand{\DY}{\mathbf{\Delta Y}}
\begin{document}

\maketitle

\section{Introduction}
\label{sec:introduction}

\paragraph{Description.}

XAI literature distinguishes between local and global interpretation methods~\cite{Molnar2020}.
Global interpretation methods aim at explaining the overall behavior of an ML model.
Many of these global interpretation methods are confounded by feature interactions,
meaning that they can produce misleading explanations when feature interactions are present.
In these cases, they often average individual effects of local interpretation methods and thereby obfuscate heterogeneous effects induced by feature interactions~\cite{Herbinger2022}.

This so-called aggregation bias~\cite{Mehrabi2019} is responsible for producing global explanations that often conceal that individual instances may deviate from the global explanation.
Therefore, global methods must quantify and inform about \textit{the uncertainty of the global explanation}, i.e., how certain we are that a global explanation is valid if applied to a local individual drawn at random.
The uncertainty of the global explanation emerges from the natural characteristics of the experiment, i.e.,~the data generating distribution and the black-box function.

In real ML scenarios, we do not know the data generating distribution for computing the expectations and the uncertainty.
The local effects are estimated from the training set's limited instances, and the global effect is computed by aggregating them.
Many methods, such as ALE, require an appropriate grouping of samples (partitioning of the feature space) for aggregating local effects that are as homogeneous as possible.
If this grouping is not done correctly, it may erroneously mix heterogeneous local effects, not due to the unavoidable heterogeneity of the data generating mechanism but due to improper grouping.

ALE is a SotA method for measuring the global feature effect, but so far, it has two limitations.
First, it does not quantify the uncertainty of the global explanation.
Second, like most global explanation methods, it requires grouping together samples before computing the global effect.
So far, this grouping is done by blindly splitting the feature space in \(K\) fixed-size non-overlapping intervals, where \(K\) is a hyperparameter provided by the user.

In this paper, first, we extend the ALE definition for quantifying the uncertainty of the global explanation.
Second, we readjust ALE to work for variable-size bins and formulate the partitioning in non-overlapping intervals as an unsupervised clustering problem.
In the unsupervised clustering case, we minimize an objective which has as lower-bound the (unavoidable) heterogeneity, i.e.\ the aggregated uncertainty of the global explanation.
Therefore, we aim to minimize the added uncertainty induced by the wrong grouping of samples.
In other words, we aim to find the optimal grouping that adds no uncertainty over the unavoidable heterogeneity.
We finally solve the minimization problem by finding the global optimum using dynamic programming.
Our method works out of the box without requiring any input by the user.
We provide a theoretical and empirical evaluation of our method.
% In all cases, to estimate
% \textit{the uncertainty of the approximation}~(\cite{Molnar2022}),
% i.e., how certain we are that the limited-samples Monte-Carlo
% approximates well the expectation over the data. These two types of
% uncertainty have not been introduced yet for ALE~(\cite{Apley2020}), a
% SotA method for measuring the global feature effect.

\paragraph{Problem Statement - Contribution.} The contribution of our
paper in bullets:

\begin{itemize}
\item Reformulation of ALE method to quantify the uncertainty of the
  global explanation
\item Formal definition of the variable-size interval splitting as an
  unsupervised clustering problem
\item Method for finding a global optimum in the clustering
  problem
\item Theoretical evaluation of our method (e.g.~show that the
  objective's lower bound is the unavoidable heterogeneity due to the
  characteristics of the problem, highlight specific cases,
  e.g.\ specific generative distribution and specific black-box function)
\item Empirical evaluation of the method in artificial and real
  datasets
\end{itemize}

\section{Mathematical formulation}
\label{sec:mathematical-formulation}

\paragraph{Effect at point \(x_s\).} In the intro, we described the
\textit{uncertainty of the global explanation} as a metric of how
certain we are that the global explanation is valid if applied to a
randomly-drawn individual.
ALE plots measure the \(s\)-th feature
effect at point \(x_s\) as the \textbf{expected} change in the output \(y\), if
we slightly change the value of the feature of interest \(x_s\):

\begin{equation}
  \label{eq:ALE_mean}
  \mu(x_s) = \mathbb{E}_{\xc|x_s}\left [\dfdx (x_s, \xc)\right ]
\end{equation}

\noindent
We model the s-th feature effect at point \(x_s\) (change in the output \(y\) wrt a slight change in the feature of interest \(x_s\)) as the random variable $\DY ; x_s $.
For notation convenience, we will refer to the s-th feature effect as \(\DY\), ommiting the \(;xs\) part.
The randomness has its origins in the ignorance of the values of the rest of the features, denoted with \(\mathbf{X}_c\).
Therefore, the s-th feature effect is defined as:

\begin{equation}
  \label{eq:fe_rv}
  \DY = g(x_s) = \dfdx (x_s, \mathbf{X}_c)
\end{equation}
%
As shown in Eq.~\eqref{eq:ALE_mean}, ALE is only interested in the expected value of \(\DY\).
Instead, we are also interested in the variance of \(\DY\) for measuring the uncertainty of the local change:

\begin{equation}
  \label{eq:ALE_var}
  \sigma^2(x_s) = \mathrm{Var}_{\xc|x_s}\left [\dfdx (x_s, \xc) \right ]
\end{equation}

\noindent
The variance in Eq.~\eqref{eq:ALE_var} informs us about the heterogeneous
effects hiding behind the explanation.

\paragraph{Effect at interval \([z_{k-1}, z_k]\).}

In real scenarios, we have ignorance about the generative distribution \(\p(x_s, \mathbf{x}_c)\),
residing to Monte-Carlo approximations of \(\mu(x_s)\), \(\sigma^2(x_s)\) using the samples of the training set.
Therefore, it is impossible to estimate Eqs.~\eqref{eq:ALE_mean},~\eqref{eq:ALE_var} at the granularity of a
point \(x_s\)\, since the  possibility to observe a sample in the interval
\([x_s - h, x_s + h]\) is zero, in the limit where \(h \to 0\).
Therefore, we are obliged to estimate the local effect in larger intervals (\(h>0\)).
We refer to the expected value and the variance of the feature effect at the interval \([z_{k-1}, z_k)\), as:

\begin{equation}
  \label{eq:mu_bin}
  \mu_k = \mu(z_{k-1}, z_k) = \frac{1}{z_k - z_{k-1}} \int_{z_{k-1}}^{z_k}
  \mathbb{E}_{\xc|x_s=z}\left [\frac{\partial f}{\partial x_s} \right ] \partial z
\end{equation}

\noindent

\begin{equation}
  \label{eq:var_bin}
  \sigma^2_k = \sigma^2(z_{k-1}, z_k) = \frac{1}{z_k - z_{k-1}} \int_{z_{k-1}}^{z_k}
  \mathbb{E}_{\xc|x_s=z} \left [ (\frac{\partial
      f}{\partial x_s}(x_s, \xc) - \mu_k )^2 \right] \partial z
\end{equation}

\noindent
We prove that the interval-variance \(\sigma^2_k \) is:

\begin{equation}
 \sigma^2_k = \frac{1}{z_k - z_{k-1}} \int_{z_{k-1}}^{z_k} \sigma^2(z) + \rho^2(z) \partial z
\end{equation}
%
where \(\rho(x_s) = \mu(x_s) - \mu_k\).
The proof is at Section \ref{sec:proofs}.
We observe that the interval-variance is the mean point-variance of the points
inside the interval \([z_{k-1}, z_k]\), plus the mean of the residual term \(\rho\).
The mean point-variance is the unavoidable variance, due to the uncertainty of the global explanation.
The mean of the residual term is an extra variance (uncertainty) due to limiting the granularity of the effect at
the bin level.

\noindent

\paragraph{Approximation of effect at interval \([z_{k-1}, z_k]\).}
Eqs.~\eqref{eq:mu_bin},~\eqref{eq:var_bin} are
approximated by the instances of the training set that lie inside the
\(k\)-th interval, i.e.
\( \mathcal{S}_k = \{ \mathbf{x}^i : z_{k-1} \leq x^i_s < z_{k} \} \):

\begin{equation}
  \label{eq:mean_estimation}
  \hat{\mu}(z_{k-1}, z_k) = \frac{1}{|\mathcal{S}_k|} \sum_{i:\mathbf{x}^i \in
    \mathcal{S}_k} \left [ \dfdx(\mathbf{x}^i) \right ]
\end{equation}

\begin{equation}
  \label{eq:variance_estimation}
  \hat{\sigma}_k(z_{k-1}, z_k) = \frac{1}{|\mathcal{S}_k|} \sum_{i:\mathbf{x}^i \in
  \mathcal{S}_k} \left [ \dfdx(\mathbf{x}^i) - \hat{\mu}_k(z_{k-1}, z_k) \right ]^2
\end{equation}


\paragraph{Uncertainty of the global effect.}

Eq.~\eqref{eq:variance_estimation} gives an approximation of the
uncertainty of the bin effect.
The uncertainty of the global effect is
simply the sum of the uncertainties in the bin effects.
The
approximation is unbiased only if the points are uniformly distributed
in \([z_{k-1}, z_k]\). (TODOs: Check what happens otherwise).

\paragraph{Minimizing the uncertainty}

Solving the problem of finding (a) the optimal number of bins \(K\) and (b) the optimal bin limits for each bin \([z_{k-1}, z_k] \forall k\) to minimize:

\begin{equation}
  \label{eq:1}
  \mathcal{L} = \sum_{k=0}^K \hat{\sigma}_k(z_{k-1}, z_k)
\end{equation}
%
The constraints are that all bins must include more than \(\tau\)
points, i.e., \(|\mathcal{S}_k| \geq \tau\).

\noindent
TODOS. Show theoretically that \(\mathcal{L} \geq \int_{x_{s, \min}}^{x_{s, \max}}\sigma^2(x_s) \partial x_s\)

\paragraph{Uncertainty of the approximation.}

In all experiments, it also important to measure the uncertainty of
the approximation.
The uncertainty of the approximation can be
quantified with two approaches:

\begin{itemize}
\item Splitting the dataset in many folds and (re)estimating
  \(\hat{\mu}(z_{k-1}, z_k), \hat{\sigma}_k(z_{k-1}, z_k)\)
\item Using the central limit theorem, we can (under assumptions) say
  that the standard error of the approximation in
  eq.~\eqref{eq:mean_estimation} is
  \(\mathrm{std\_error} =
  \frac{\hat{\sigma}_k}{\sqrt{|\mathcal{S}_k|}}\).
\end{itemize}

\section{Toy Example}
\label{sec:toy-example}

\paragraph{Example 1.} We use the following example:

\begin{equation}
  \label{eq:model_1}
  \begin{gathered}
    f(x_1, x_2) = b_0 + b_1 x_1 + b_2 x_2 + b_3 x_1 x_2\\
    x_1 \sim \mathcal{U}(0,1) \\
    x_2 = x_1 + \epsilon, \epsilon \sim \mathcal{N}(\mu=0, \sigma_2^2)
  \end{gathered}
\end{equation}
%
Therefore, according to Eq.~\eqref{eq:ALE_mean}
\(\mu_(x_1) = b_1 + b_3 x_1\) and according to Eq.~\eqref{eq:ALE_var}
\(\sigma^2(x_1) = b_3^2 \sigma_2^2 \Rightarrow \sigma(x_1) = b_3
\sigma_2 \).

\paragraph{ALE with uncertainty.}

In figure~\ref{fig:bullet-1-im-3} we observe the ALE effect with
uncertainty for 3 different values of \(\sigma_2 = \{0.01, 0.1,
1\}\).
In all case the mean effect (ALE) is the same, but the
uncertainty of the global explanation quantifies the impact of the
heterogeneous effects hiding behind the global explanation.

\begin{figure}[!h]
  \centering
  \includegraphics[width=.32\linewidth]{./example_gromping_paper/ALE_gt_s2_0_01}
  \includegraphics[width=.32\linewidth]{./example_gromping_paper/ALE_gt_s2_0_1}
  \includegraphics[width=.32\linewidth]{./example_gromping_paper/ALE_gt_s2_1_0}
  \caption{ALE (a) \(\sigma_2^2 = 0.01\), (b) \(\sigma_2^2 = 0.1\), (c) \(\sigma_2^2 = 1.\)}
  \label{fig:bullet-1-im-3}
\end{figure}


\paragraph{Bin Splitting.}

In figure~\ref{fig:im2}, set up for
\(b_0=0, b_1=1, b_2=1, b_3=2, \sigma_2^2 = 0.1\).
Therefore, the
unavoidable uncertainty is \(\sigma^2(x_1) = b_3^2 \sigma_2^2 =
0.4\).
We observe that as the bins become denser the added
uncertainty due to binning becomes less.
But as the bins become denser, fewer points lie inside them and the estimation is poor.
The
vertical lines show the maximum number of bins for dataset sizes, if I
want at least 25 points per bin.


\begin{figure}[!h]
  \centering
  \includegraphics[width=.7\linewidth]{./example_gromping_paper/bin_varinace}
  \caption{}
  \label{fig:im2}
\end{figure}


\section{Evaluation}
\label{sec:evaluation}

\paragraph{Experiments.}

% Design two artificial examples to convince
% that:

% \begin{itemize}
% \item It is important to quantify the uncertainty of the explanation
%   \(\Rightarrow\) Design an example where we can obtain the
%   ground-truth ALE effect and show the change in the uncertainty as we
%   subsample the training set. Show qualitatively that the variance
%   (and the standard error) is growing as we subsample the training
%   set. Show that variance and standard error are valid compared to the
%   ground truth.
% \item It is important to split the feature axis in appropriate
%   non-overlaping bins \(\Rightarrow\) Design an example where by
%   changing the number of bins we observe that: with large bins, we
%   compute a coarse-scale ALE estimation missing fine-grain details
%   and, if ALE estimation instead of DALE, suffers from OOD
%   sampling. With small bins, the estimation of \(\mu\), \(\sigma^2\)
%   (computed only in non-empty bins and interpolated in the others) is
%   noisy. Show that with variable-size bins optimized to minimize the
%   variance we obtain the best estimation.

% \end{itemize}


\paragraph{Metrics.}

% Normalized Mean Squared Error (NMSE) between
% ground truth and approximation.


% \begin{table}[!h]
%   \centering
%   \begin{tabular}{|c|c|c|}
%     - & \texttt{ALE} vs \texttt{GT\_ALE} & ALE \\
%     \hline
%     \multirow{4}{*}{\texttt{Metric}} & \texttt{KL-Divergence} & \\
%     & \texttt{JS-Divergence} & \\
%     & \(r^2\) (\(\mu^2,\hat{\mu}^2\)) & \\
%     & \texttt{NMSE} (\(\mu^2,\hat{\mu}^2\)) & \\
%   \end{tabular}
%   \caption{Metrics that can be used}
%   \label{tab:metrics}
% \end{table}

\section{Proofs}
\label{sec:proofs}

Given that:

\[
  \sigma^2_k = \sigma^2(z_{k-1}, z_k) = \frac{1}{z_k - z_{k-1}} \int_{z_{k-1}}^{z_k}
  \mathbb{E}_{\xc|x_s=z} \left [ (\frac{\partial
      f}{\partial x_s}(x_s, \xc) - \mu_k )^2 \right] \partial z
\]


\[
\sigma^2(x_s) = \mathrm{Var}_{\xc|x_s}\left [\dfdx (x_s, \xc) \right ]
\]

\[
\(\rho(x_s) = \mu(x_s) - \mu_k\)
\]


We want to prove that:

\[
 \sigma^2_k = \frac{1}{z_k - z_{k-1}} \int_{z_{k-1}}^{z_k} \sigma^2(z) + \rho^2(z) \partial z
\]

Proof:


\begin{gather}
  \sigma^2_k = \sigma^2(z_{k-1}, z_k) = \frac{1}{z_k - z_{k-1}} \int_{z_{k-1}}^{z_k}
  \mathbb{E}_{\xc|x_s=z} \left [ (\frac{\partial f}{\partial x_s}(z, \xc) - \mu_k )^2 \right] \partial z \\
  = \frac{1}{z_k - z_{k-1}} \int_{z_{k-1}}^{z_k}
  \mathbb{E}_{\xc|x_s=z} \left [ (\frac{\partial f}{\partial x_s} - \mu(z) + \rho(z) )^2 \right] \partial z \\
  = \frac{1}{z_k - z_{k-1}} \int_{z_{k-1}}^{z_k} \left(
  \mathbb{E}_{\xc|x_s=z} \left [ (\frac{\partial f}{\partial x_s} - \mu(z) )^2 \right ]  +
  \mathbb{E}_{\xc|x_s=z} \left [ \rho(z)^2 \right] +
  \mathbb{E}_{\xc|x_s=z} \left [ 2(\frac{\partial f}{\partial x_s} - \mu(z) )\rho(z) \right ] \right )  \partial z \\
  = \frac{1}{z_k - z_{k-1}} \int_{z_{k-1}}^{z_k} \left (
  \sigma^2(x_s)  + \rho^2(z) + 2 ( \mu(z) -  \mu(z) )\rho(z) \right )  \partial z \\
  = \frac{1}{z_k - z_{k-1}} \int_{z_{k-1}}^{z_k} \sigma^2(x_s)  + \rho^2(z) \partial z \\
\end{gather}



\bibliographystyle{plain}
\bibliography{summary_bibliography}

\end{document}