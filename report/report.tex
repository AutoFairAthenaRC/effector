\documentclass[12pt]{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{bbm}
\usepackage{graphicx}
\usepackage{geometry}[margin=1in]
\usepackage{subcaption}

% for algorithms
\usepackage[linesnumbered,ruled]{algorithm2e}

\usepackage{natbib}
\bibliographystyle{plainnat}
% set path for bibliography

\title{Regionally Additive Models: Explainable-by-design models minimizing feature interactions}

\newcommand{\dfdx}{\frac{\partial f}{\partial x_s}}
\newcommand{\Rd}{\mathbb{R}^d}
\newcommand{\xb}{\mathbf{x}}
\newcommand{\xc}{\mathbf{x_c}}
\newcommand{\xcc}{\mathbf{x_{/s}}}
\newcommand{\fxc}{f^{(\xc)}}
\newcommand{\fxs}{f^{(x_s)}}
\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Ycal}{\mathcal{Y}}
\newcommand{\when}[1]{\mathbbm{1}_{#1}}

\author{Vasilis Gkolemis}


\begin{document}
\maketitle

\begin{abstract}
Generalized Additive Models (GAMs) are a popular class of explainable-by-design models that are widely used in practice.
GAMs are based on the assumption that the effect of each feature on the target is independent of the values of the
other features, however, in cases where this assumption is violated they may lead to poor performance.
To address this limitation we propose Regionally Additive Models (RAMs), a novel class of explainable-by-design models,
that fits multiple GAMs to subregions of the feature space where interactions are minimized.
Our approach consists of two steps: first, we fit a black-box model and we identify the subregions where the black-box model is nearly locally additive,
i.e., where the effect of each feature on the target is independent of the values of the other features.
Secondly, we train a GAM specifically for each identified subregion.

We show that RAMs are more expressive than GAMs while they are still interpretable.

\end{abstract}

\section{Introduction and Related Work}


% Paragraph for Motivating about Regionally Additive Models
Generalized Additive Models (GAMs) are a widely recognized class of explainable-by-design models.
Their popularity stems from their seamless interpretability; since they are a linear (additive) combination of univariate functions,
\(f(\xb) = c + \sum_{s=1}^D f_s(x_s)\), each individual univariate function can be readily visualized and comprehended in isolation.
However, the expressiveness of GAMs is limited when it comes to learn interactions between features.
To overcome this limitation, a family of models extends GAMs enabling pairwise interactions,
i.e., \(f(\xb) = c + \sum_{s=1}^D f_s(x_s) + \sum_{s=1}^D \sum_{c \neq s} f_{sc}(x_s, x_c)\).
Pairwise interactions can also be visualized and understood in isolation, so these models also maintain their explainable-by-design nature.
Unfortunately, this does not hold for any interaction that involves more than two features, thus, the expressiveness of GAMs is limited to capturing up to two-feature interactions.

% Paragraph for Motivating about Regionally Additive Models
To overcome this limitation, we propose Regionally Additive Models (RAMs), a novel class of explainable-by-design models,
that fits multiple GAMs to subregions of the feature space where interactions are minimized.
Our approach consists of a three-step pipeline.
First, we fit a black-box model to capture all high-order interactions.
Second, we identify the subregions where the black-box model is nearly locally additive.
Finally, we train a GAM specifically for each identified subregion.

\section{Motivation}
\label{sec:motivation}

Consider the black-box function \(f(\xb) = 8x_2\when{x_1 > 0}\when{x_3=0}\)
with \(x_1, x_2 \sim \mathcal{U}(-1,1)\) and \(x_3 \sim Bernoulli(0,1)\).
Although very simple, the existence of the interaction term $8x_2\when{x_1 > 0}\when{x_3=0}$ that involves
three features $x_1, x_2, x_3$ makes it impossible to be learned by a GAM.
As we see in Figure~\ref{subfig:global_gam}, a GAM misleadingly learns that $f(x_2) = 2x_2$,
because in $\frac{1}{4}$ of the cases ($x_1 > 0 \text{ and } x_3 = 0$) the impact of $x_2$ to the output is $8x_2$,
and in the rest $\frac{3}{4}$ of the cases the impact of $x_2$ to the output is $0$.
However, if splitting the input space in two subregions we observe that \(f\) is additive in each one (regionally additive):
%
\begin{equation}
    \label{eq:regionally_additive}
    f(\xb) = \begin{cases} 8x_2 & \text{if } x_1 > 0 \text{ and } x_3 = 1 \\ 0 & \text{otherwise} \end{cases}
\end{equation}
%
Therefore, if we knew the appropriate subregions for $x_2$,
namely, \(\mathcal{R}_{21} = \{x_1 > 0 \text{ and } x_3 = 0\}\)
and  \(\mathcal{R}_{22} = \{x_1 \leq 0 \text{ or } x_3 = 1\}\),
we could fit the following model to the data

\begin{equation}
    \label{eq:regional_gam}
    f^{\mathtt{RAM}}(\xb) = f_1(x_1) + f_{21}(x_2) \when{(x_1, x_3) \in \mathcal{R}_{21}} + f_{22}(x_2) \when{(x_1, x_3) \in \mathcal{R}_{22}} + f_3(x_3)
\end{equation}
%
Equation~\eqref{eq:regional_gam} represents a Regionally Additive Model (RAM). A RAM is nothing more than a GAM fitted on each subregion of the feature space.
Importantly, RAM's enhanced expressiveness does not come at the expense of interpretability.
We can still visualize and comprehend each univariate function in isolation, exactly as we would do with a GAM,
with the only difference being that we have to consider the subregions where each univariate function is active, as we observe in
Figures~\ref{subfig:regional_gam_1} and~\ref{subfig:regional_gam_2}.
The key challenge of RAMs is to appropriately identify the subregions where the black-box function is (close to) regionally additive.
For this purpose, as we will see in Section~\ref{subsec:regional_effect_methods}, we propose a novel algorithm that is based on the idea of
regional effect plots.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/global_GAM}
        \caption{\(f_2(x_2)\)}
        \label{subfig:global_gam}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/regional_gam_subreg_1}
        \caption{\(f_2(x_2) \when{x_1 > 0 \text{ and } x_3 = 1}\)}
        \label{subfig:regional_gam_1}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/regional_gam_subreg_2}
        \caption{\(f_2(x_2) \when{x_1 \leq 0 \text{ or } x_3 \neq 1}\)}
        \label{subfig:regional_gam_2}
    \end{subfigure}
    \caption{Caption for the entire figure}
    \label{fig:ram_example}
\end{figure}


\section{The RAM framework}

\subsection{Background}

Let \(\Xcal \in \Rd\) be the \(d\)-dimensional feature space, \(\Ycal\) the target space and \(f(\cdot) : \Xcal \rightarrow \Ycal\) the black-box function.  We use index \(s \in \{1, \ldots, d\}\) for the feature of interest and \(c = \{1, \ldots, d\} - s\) for the rest.
For convenience, we use \((x_s, \xcc)\) to refer to \((x_1, \cdots , x_s, \cdots, x_D)\) and, equivalently, \((X_s, X_c)\) instead of \((X_1, \cdots , X_s, \cdots, X_D)\) when we refer to random variables.
The training set \(\mathcal{D} = \{(\xb^i, y^i)\}_{i=1}^N\) is sampled
i.i.d.\ from the distribution \(\mathbb{P}_{X,Y}\).  Finally,
\(f^{\mathtt{<method>}}(x_s)\) denotes how \(\mathtt{<method>}\)
defines the feature effect and \(\hat{f}^{\mathtt{<method>}}(x_s)\)
how it estimates it from the training set.

\subsection{Fit a black-box function}

\subsubsection{Objective}

In the first step of the pipelin, a black-box function \(f(\cdot)\) is fitted to the training set \(\mathcal{D} = \{(\xb^i, y^i)\}_{i=1}^N\).
In principle, the black-box function can be any model that is expressive enough to accurately fit the data,
meaning that it is able to learn the underlying mapping \(\mathbb{P}_{Y|X}\).

\subsubsection{Proposed Approach}

For being able to use the DALE approximation that we propose in the next step,
the black-box function should be differentiable.
Recent developments have shown that differentiable Deep Learning models specifically designed for tabular
data~\citep{arik2021tabnet} can achieve state-of-the-art performance, make them a good candidate for this step.
In the running example, we use a simple neural network with three hidden layers as the black-box function,
which achieves a test \(\texttt{MSE} \approx 0.01\).
The neural network is trained using the Adam optimizer (add citation) with a learning rate of \(0.01\).
Based on the small test \(\texttt{MSE}\), we can assume that the neural network is able to capture any interactions
between the features.

\subsection{Regions that minimize feature interactions}
\label{subsec:regional_effect_methods}

In this step, we use regional effect methods to identify the regions where the black-box function is nearly locally additive.

\subsubsection{Objective}

% Describe the goal of the regional effect methods
Regional effect methods yield for each individual feature \(s\), a set of \(T_s\) non-overlapping regions,
denoted as \(\{\mathcal{R}_{st}\}_{t=1}^{T_s}\) where \(\mathcal{R}_{st} \subseteq \Xcal_{/s}\).
We denote the number of non-overlapping regions for feature \(s\) as \(T_s\), because for each feature \(s\),
the number of regions \(T_s\) can be different.
The primary objective is to identify regions in which the impact of the \(s\)-th feature on the output is
\textit{relatively independent} of the values of the other features \(\xcc\).
To better grasp this objective, if we decompose the impact of the \(s\)-th feature on the output $y$ into two terms:
\(f_s(x_s, \xcc) = f_{s,eff}(x_s) + f_{s, int}(x_s, \xcc)\),
where \(f_{s,eff}(\cdot)\) represents the independent effect
and \(f_{s, int}(\cdot)\) represents the interaction effect,
the objective is to identify regions \(\{\mathcal{R}_{st}\}_{t=1}^{T_s}\) where the interaction effect is minimized.
It is important to note that the union of all regions \(\{\mathcal{R}_{st}\}_{t=1}^{T_s}\) covers the entire feature space \(\Xcal_{/s}\).
Regionally Additive Models (RAM) formulation is:

\begin{equation}
\label{eq:ram_formulation}
f^{\mathtt{RAM}}(\xb) = c + \sum_{s=1}^D \sum_{t=1}^{T_s} f_{st}(x_s) \when{\xcc \in \mathcal{R}_{st}}
\end{equation}
%
In the above formulation, \(f_{st}(\cdot)\) is the component of the \(s\)-th feature which is active on the \(t\)-th region.
RAM can be viewed as a GAM with \(T_s\) components per feature where each component is applied to a specific region \(\mathcal{R}_{st}\).
To facilitate this formulation, we introduce a new feature space \(\Xcal^\mathtt{RAM}\) defined as:

\begin{equation}
\label{eq:ram_feature_space}
\begin{aligned}
\Xcal^{\mathtt{RAM}} &= \{x_{st} | s \in \{1, \ldots, D\}, t \in \{1, \ldots, T_s\}\} \\
x_{sk} &= \begin{cases}
x_s, & \text{if } \xcc \in \mathcal{R}_{sk} \\
0, & \text{otherwise}
\end{cases}
\end{aligned}
\end{equation}
%
Finally, we can simply formulate RAM as a typical GAM on the extended feature space \(\Xcal^{\mathtt{RAM}}\):

\begin{equation}
\label{eq:ram_formulation2}
f^{\mathtt{RAM}}(\xb) = c + \sum_{s,t} f_{st}(x_{st})
\end{equation}
%
In the running example, to minimize the effect of feature interactions, we have to split the effect of feature \(x_2\) into two regions,
\(\mathcal{R}_{21} = \{x_1 > 0 \text{ and } x_3 = 1\}\) and \(\mathcal{R}_{22} = \{x_1 \leq 0 \text{ or } x_3 = 0\}\).
The RAM formulation is \(f^{\mathtt{RAM}}(\xb) = f_1(x_1) + f_{21}(x_{21}) + f_{22}(x_{22}) + f_3(x_3)\).


\subsubsection{Proposed Approach}

To identify the regions of the input space where the impact of feature interactions is reduced,
we have developed a regional effect method influenced by the research conducted by
\citet{herbinger2023decomposing} and \citet{gkolemis2023dale}.
\citet{herbinger2023decomposing} introduced a versatile framework for detecting such regions,
where one of the proposed methods is the Accumulated Local Effects~\citep{apley2020visualizing}.
We have adopted their approach with two notable modifications.
First, instead of using the ALE plot, we employ the Differential ALE (DALE) method introduced by \citet{gkolemis2023dale},
which provides considerable computational advantages when the underlying black-box function is differentiable.
Second, we utilize variable-size bins, instead of the fixed-size ones in DALE, because the result in a more accurate approximation.

Given a black-box function \(f(\cdot)\) and a dataset \(\mathcal{D} = \{(\xb^i, y^i)\}_{i=1}^N\),
the DALE plot is defined as:

\begin{equation}  \label{eq:DALE_accumulated_mean_est}
  \hat{f}^{\mathtt{DALE}}(x_s) = \Delta x \sum_{k=1}^{k_x} \underbrace{\frac{1}{|\mathcal{S}_k|} \sum_{i:\mathbf{x}^{(i)} \in
    \mathcal{S}_k} \dfdx(\mathbf{x}^i)}_{\hat{\mu}(z_{k-1}, z_k)})
\end{equation}
%
where \(k_x\) the index of the bin such that
\(z_{k_x-1} \leq x_s < z_{k_x} \) and \(\mathcal{S}_k\)
is the set of the instances of the \(k\)-th bin, i.e.
\( \mathcal{S}_k = \{ \xb^i : z_{k-1} \leq x^{(i)}_s < z_{k} \} \).
DALE computes the average effect (impact) of the feature \(x_s\) on the output of the black-box function \(f(\cdot)\).
The average effect is computed by dividing the feature space into bins and computing the average effect of
the feature \(x_s\) in each bin.
In cases where there are interactions between the features, the instance-level effects inside each bin
start to deviate from the average effect.
We can measure such deviation using the standard deviation of the instance-level effects inside each bin:
\begin{equation}
  \label{eq:var_bin_approx}
  \hat{\sigma}^2(z_{k-1}, z_k) = \frac{1}{|\mathcal{S}_k| - 1}
\sum_{i:\mathbf{x}^i \in \mathcal{S}_k} \left ( \dfdx(\mathbf{x}^i) -
  \hat{\mu}(z_{k-1}, z_k) \right )^2
\end{equation}
%
and the global interaction between the feature \(x_s\) and the rest of the features with the
aggregated bin-deviation \({\left ( \sum_{k=1}^{k_x} (z_k - z_{k-1})^2 \hat{\sigma}^2(z_{k-1}, z_k) \right)}^{\frac{1}{2}} \).
Therefore, in order to minimize the interaction we search for a set of regions \(\{\mathcal{R}_{st}\}_{t=1}^{T_s}\),
that minimize the aggregated bin-deviation defined on the subregions.
For this reason we define the bin-effect, the bin-deviation and the aggregated deviation on a subregion \(\mathcal{R}_{st}\) as:

\begin{equation}
    \label{eq:bin_effect_subregion}
    \hat{\mu}_{\mathcal{R}_{st}}(z_{k-1}, z_k) = \frac{1}{|\mathcal{S}_k \cap \mathcal{R}_{st}|} \sum_{i:\mathbf{x}^i \in \mathcal{S}_k \cap \mathcal{R}_{st} } \dfdx(\mathbf{x}^i)
\end{equation}

\begin{equation}
  \label{eq:bin_deviation_subregion}
  \hat{\sigma}_{\mathcal{R}_{st}}^2(z_{k-1}, z_k) = \frac{1}{|\mathcal{S}_k \cap \mathcal{R}_{st}| - 1}
\sum_{i:\mathbf{x}^i \in \mathcal{S}_k \cap \mathcal{R}_{st} } \left ( \dfdx(\mathbf{x}^i) -
    \hat{\mu}(z_{k-1}, z_k) \right )^2
\end{equation}

\begin{equation}
  \label{eq:SE}
  \mathcal{L}_{\mathcal{R}_{sk}} = \sum_{k=1}^K (z_k - z_{k-1})^2 \hat{\sigma}_{\mathcal{R}_{st}}^2(z_{k-1}, z_k)
\end{equation}

The goal is to find the optimal set of subregions \(\{\mathcal{R}_{st}\}_{t=1}^{T_s}\) that minimize
the aggregated deviation. For this reason, we define the following optimization problem:

\begin{equation}
  \label{eq:optimal_subregions}
  \begin{aligned}
    & \underset{\{\mathcal{R}_{st}\}_{t=1}^{T_s}}{\text{minimize}}
    & & \sum_{t=1}^{T_s} \mathcal{L}_{\mathcal{R}_{st}} \\
    & \text{subject to}
    & & \bigcup_{t=1}^{T_s} \mathcal{R}_{st} = \mathcal{X} \\
    & & & \mathcal{R}_{st} \cap \mathcal{R}_{s\tau} = \emptyset, \quad \forall t \neq \tau
  \end{aligned}
\end{equation}

For minimizing Eq.~\eqref{eq:SE}, we set up a CART-like algorithm, as proposed by \citet{herbinger2023decomposing}.
The algorithm is described in Algorithm~\ref{alg:subregion_identification}.


\begin{algorithm}
\caption{DALE-based Region Detection}
\label{alg:subregion_identification}
\SetAlgoLined
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\BlankLine
\Input{X, J, T}
\BlankLine
\Output{optimal\_splits}
\BlankLine
splits[s, c, t] $\gets$ None\;
positions[s, c, t] $\gets$ None\;
\For{$s = 1$ to $D$}{
    \For{$c \in \{1, \dots, D\} \setminus \{s\}$}{
        is\_cat = True/False \;
        X\_list $\gets [X]$\;
        J\_list $\gets [J]$\;
        \For{$t = 1$ to $T$}{
            L, position, X\_split, J\_split $\gets$ BestSplit(X\_list, J\_list, s, is\_cat)\;
            splits[s, c, t] $\gets$ L\;
            positions[s, c, t] $\gets$ position\;
        }
    }
}
\end{algorithm}


\begin{algorithm}
\caption{BestSplit}
\label{alg:single_feature_subregion}
\SetAlgoLined
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}

\Input{X\_list, J\_list, s, c, is\_cat}
\Output{BestSplits}

% Initialize variables
\BlankLine
positions $\gets$ GetPositions(X\_list, c, is\_cat)\;
L $\gets$ []\; % \Comment{List to store interaction values}
\For{p \textnormal{in} positions}{
    X\_split, J\_split $\gets$ SplitDataset(X\_list, J\_list, c, p, is\_cat)\;
    L $\gets$ GetInteraction(X\_split, J\_split)\;
}
split\_position = argmin(L)\;
position $\gets$ positions[split\_position]\;
X\_split, J\_split $\gets$ SplitDataset(X\_list, J\_list, c, position, is\_cat)\;
\Return{L, position, X\_split, J\_split}
\BlankLine
\end{algorithm}


\begin{algorithm}
\caption{SplitDataset}
\label{alg:split_dataset}
\SetAlgoLined
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{X\_list, J\_list, c, val, is\_cat}
\Output{X\_split, J\_split}
X\_split $\gets$ []\;
J\_split $\gets$ []\;
\For{i = 1 to len(X\_list)}{
    X= X\_list[i]\;
    J= J\_list[i]\;
    \uIf{is\_cat}{
        ind\_1 $\gets$ X[:, c] $=$ val\;
        ind\_2 $\gets$ X[:, c] $\neq$ val\;
    }
    \Else{
        ind\_1 $\gets$ X[:, c] $\leq$ val\;
        ind\_2 $\gets$ X[:, c] $>$ val\;
    }
    Append X[ind\_1], X[ind\_2] to X\_split\;
    Append J[ind\_1], J[ind\_2] to J\_split\;
}
\Return X\_split, J\_split
\end{algorithm}

\begin{algorithm}
\caption{GetInteraction}
\label{alg:get_interaction}
\SetAlgoLined
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{$X\_list$, $J\_list$, $min\_points$}
\Output{weighted average of interaction levels}
N $\gets$ total number of items in $X\_list$\;
W $\gets$ []\;
L $\gets$ []\;
\For{$i$ in len($X\_list$)}{
    X $\gets$ X\_list[i]\;
    J $\gets$ J\_list[i]\;
    \uIf{$|X|$ < min\_points}{
      Append $\infty$ to L\;
    }
    \Else{
        Append $\mathcal{L}(X, J)$ to $L$\;
    }
    Append $\frac{|X|}{N}$ to $W$\;
}
\Return $\text{sum}(L \cdot W)$\;
\end{algorithm}


\subsection{Fitting the GAMs}

\subsubsection{Objective}

\subsubsection{Proposed Approach}

\subsection{Discussion}

Recently, a number of methods have been proposed to extend traditional GAMs and make them more expressive.
The majority of the ideas follow one of the following research directions;
The first one targets on representing the main components of a GAM $\{ f_i(x_i) \}$ with novel models.
For example, \citet{agarwal2021neural} who used an end-to-end neural network for learning the main components.
The second one focuses on extending the GAMs to model interactions between the features.
For example, \citet{lou2013accurate} proposed Explainable Boosting Machines (EBMs) which are a generalized additive model with pairwise interaction terms.
It is worth noting that the proposed method is orthogonal to the aforementioned research directions.
As we will show in the experiments, the proposed method can be used in conjunction with any of the
aforementioned methods to improve the accuracy of the resulting model, while maintaining the interpretability of the model.

\section{Experiments}

\subsection{Synthetic Examples}

\subsection{Real-World Datasets}



\bibliography{report}

\section{Appendix}

    \subsubsection{Algorithmic Details of Subregion Detection}


\end{document}
