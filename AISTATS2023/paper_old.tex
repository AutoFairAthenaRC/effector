\documentclass[twoside]{article}

\usepackage{aistats2023}
% If your paper is accepted, change the options for the package
% aistats2023 as follows:
%
%\usepackage[accepted]{aistats2023}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.

% If you set papersize explicitly, activate the following three lines:
%\special{papersize = 8.5in, 11in}
%\setlength{\pdfpageheight}{11in}
%\setlength{\pdfpagewidth}{8.5in}

% If you use natbib package, activate the following three lines:
\usepackage[round]{natbib}
\renewcommand{\bibname}{References}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}
\bibliographystyle{plainnat}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xfrac}
\usepackage{hyperref}
\usepackage{graphicx}
\graphicspath{ {./../examples/} }
% If you use BibTeX in apalike style, activate the following line:
%\bibliographystyle{apalike}

\usepackage{tikz}
\usetikzlibrary{matrix,positioning,arrows.meta,arrows,fit,backgrounds,decorations.pathreplacing}
\tikzset{ mymat/.style={ matrix of math nodes, text height=2.5ex, text
depth=0.75ex, text width=6.00ex, align=center, column
sep=-\pgflinewidth, nodes={minimum height=5.0ex} }, mymats/.style={
mymat, nodes={draw,fill=#1} }, mymat2/.style={ matrix of math nodes,
text height=1.0ex, text depth=0.0ex, minimum width=5ex, % text
width=7.00ex, align=center, column sep=-\pgflinewidth }, }
\usetikzlibrary{shapes.geometric, arrows, backgrounds, scopes}
\usepackage{pgfplots} \pgfplotsset{width=6.75cm, compat=newest}
\usepackage[utf8]{inputenc} \DeclareUnicodeCharacter{2212}{âˆ’}
\usepgfplotslibrary{groupplots,dateplot}
\usetikzlibrary{patterns,shapes.arrows}



\newcommand{\dfdx}{f^s}
\newcommand{\xc}{\mathbf{x}_c}
\newcommand{\DY}{\mathbf{\Delta Y}}
\newcommand{\xb}{\mathbf{x}}
\newcommand{\Xcb}{\mathbf{X}_c}
\newcommand{\Xb}{\mathcal{X}}
\newcommand{\todo}[1]{[\textcolor{red}{#1}]}

\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\twocolumn[

\aistatstitle{Uncertainty-aware Accumulated Local Effects (UALE) for quantifying the heterogeneity of instance-level feature effects}

\aistatsauthor{ Author 1 \And Author 2 \And  Author 3 }

\aistatsaddress{ Institution 1 \And  Institution 2 \And Institution 3 } ]

\begin{abstract}

Accumulated Local Effects (ALE) is a popular explainable AI method that quantifies how a feature influences the decisions of a model, handling well correlated features. However, in case of strong interactions between features, instance-level feature effects may deviate from the average curve provided by ALE. Therefore, it is crucial to quantify this deviation, namely, the uncertainty of the effect. In this work, we define Uncertainty-aware ALE (UALE) to quantify and visualize, on a single plot, both the average effect and the uncertainty. Furthermore, as in ALE, UALE's approximation requires partitioning the feature domain into non-overlapping intervals (bin-splitting). The average effect and the uncertainty are then computed from the instances that lie in each bin.  In this work, we formally prove that to achieve an unbiased approximation of the uncertainty in each bin, bin-splitting must follow specific constraints. Based on this, we propose a method for automatically finding the optimal intervals, balancing the trade-off between estimation bias and variance~\todo{check if this phrase explains well the concept.} We demonstrate through synthetic and real datasets (a) the advantages of modeling the uncertainty with UALE compared to other methods and (b) the importance of appropriate bin splitting for an accurate approximation of the average effect and uncertainty.
\end{abstract}


\section{INTRODUCTION}

Recently, Machine Learning (ML) has been adopted in critical domains,
such as healthcare and finance. In these areas, we need a combination
of accurate predictions along with meaningful explanations to support
them. For this reason, there is an increased interest in Explainable
AI (XAI) to understand the decision mechanism of complex black-box
models. XAI literature distinguishes between local and global
techniques~\citep{Molnar2020interpretable}. Local methods provide
instance-level explanations, i.e.~explain the prediction for a
specific instance, whereas global methods summarize the entire model
behavior.

Global methods create a global explanation by aggregating the
instance-level explanations into a single interpretable outcome,
usually a number or a plot.  A popular class of global methods are the
so-called Feature Effect (FE) methods~\citep{Gromping2020MAEP} that
isolate the impact of a single feature on the output\footnote{FE
  methods also isolate the combined effect of a pair of features to
  the output. Combinations of more than two features are uncommon,
  since they are difficult to estimate and visualize.}. There are
three widely-used FE methods: \emph{Partial Dependence Plots}
(PDP)\citep{friedman2001greedy}, \emph{Marginal Plots}
(MP)\citep{apley2020visualizing} and \emph{Aggregated Local Effects}
(ALE)\citep{apley2020visualizing} with ALE being established as the
state-of-the-art for measuring the average effect, since PDP and MP
have been criticized~\citep{Gromping2020MAEP} to compute erroneous
effects when the input features are (highly) correlated.

However, when strong interactions between features exist, the
instance-level feature effects may significantly deviate from the
aggregated outcome of ALE~\citep{herbinger2022repid}, a phenomenon
called \emph{aggregation bias}~\citep{mehrabi2021survey}.  Aggregation
bias leads to ambiguities; for example, a feature with zero average
effect may indicate no effect on the output or a highly positive
effect for some instances and a highly negative effect for
others. Such ambiguities could be resolved if not only the average
effect but also the deviation of instance-level feature effects, a
quantity we refer to as the \emph{uncertainty of the effect}, from the
average effect is provided.

%Therefore, global methods should quantify not only the aggregated (averaged) effect, but also the level of heterogeneity of instance-level explanations.  


%There are three widely-used FE methods: \emph{Partial Dependence Plots} (PDP)\citep{friedman2001greedy}, \emph{Marginal Plots}
%(MP)\citep{apley2020visualizing} and \emph{Aggregated Local Effects}
%(ALE)\citep{apley2020visualizing}. ALE has been established as the
%state-of-the-art for measuring the average effect, since PDP and MP
%have been criticized~\citep{Gromping2020MAEP} to compute erroneous
%effects when the input features are (highly) correlated. 

In this work, we propose UALE, a global feature-effect method that
extends ALE by quantifying the uncertainty associated with the average
effect. Similarly to ALE, UALE's approximation requires partitioning
the feature domain into non-overlapping intervals
(bin-splitting). However, as we formally prove, a fixed equi-width
partitioning without considering the characteristics of the
instance-level effects may lead to an erroneous (biased) estimation of
the uncertainty. Therefore, we propose an automatic uncertainty-driven
partitioning that secures an unbiased approximation of the
uncertainty.

%However, ALE faces two limitations, the first concerns ALE definition and the second ALE approximation. Regarding the definition, ALE does not inform the user about the uncertainty. 
\color{blue}\todo{This should go to related work}In contrast, in PDP the
heterogeneity of instance-level effects can be identified by exploring
the Individual Conditional Expectations
(ICE)~\citep{goldstein2015peeking}. \color{black}
%Regarding the approximation, ALE requires partitioning the feature domain into non-overlapping intervals (bin-splitting) and estimate the effect inside each interval (bin-effect) from the training set instances that fall inside. 
\color{blue}\todo{to be moved in related work}
Finding
an appropriate sequence of bins is of particular importance, since
ALE's interpretation is meaningful only inside each
bin~\citep{molnar2022}. However, this crucial step has not raised the
appropriate attention. With the current
approaches~\citep{apley2020visualizing, gkolemis22}, the domain is
split \emph{blindly}, i.e.~without considering the characteristics of
the instance-level effects, in \(K\) equisized bins.\color{black}

%In this paper, we present Uncertainty-aware Accumulated Local Effects(UALE), a method that visualizes in a single plot both the av erage feature effect and the uncertainty, i.e.~the level of heterogeneity of instance-level effects. The average effect is defined as in ALE and the uncertainty inherits ALE's advantages, i.e.~takes into account the correlations between features. We also propose an accurate UALE approximation. We formally prove that for an unbiased estimation of the uncertainty, the partitioning of the feature domain must respect specific conditions. Using that, we set-up an optimization problem where we search for a partitioning, a sequence of \emph{variable-size}intervals, (a) with enough instances (accurate estimation) and (b) that respect the above conditions (unbiased estimation). Finally, we propose a method for automatically finding the optimal partitioning. We apply our method in multiple synthetic and real world datasets for evaluating its performance. 
The contributions of this
paper are:
\begin{itemize}
\item A global feature effect method that quantifies both the average
  effect and the uncertainty, i.e.,~the heterogeneity of instance level
  effects.
\item An unbasied approximation of the uncertainty through a dynamic uncertainty-driven bin partitioning of the feature domain.
%A formal proof of the conditions the feature domain partitioning must fulfill for an unbiased approximation of the uncertainty.
\item A thorough evaluation on both synthetic and real datasets.
%An accurate UALE approximation by automatically partitioning into variable-size intervals that respect the above conditions.
\end{itemize}

The implementation of our method and the code for
reproducing all the experiments is provided in the submission and will
become publicly available upon acceptance.

\section{BACKGROUND AND RELATED WORK}

We here review the existing methods for modeling the average effect
and the heterogeneity of instance-level effects. The necessary
background for ALE~\citep{apley2020visualizing} is presented in
Section~\ref{sec:feat-effect-meth}, whereas ALE
approximations~\citep{apley2020visualizing, gkolemis22} are presented
in Section~\ref{sec:ale-approximation}.

\paragraph{Notation} 

Let \(\mathcal{X} \in \mathbb{R}^d\) be the d-dimensional feature space, \(\mathcal{Y} \in \mathbb{R}\) the target space and \(f(\cdot) : \mathcal{X} \rightarrow \mathcal{Y}\) the black-box function. We use index \(s \in \{1, \ldots, d\}\) for the feature of interest and \(c \in \{1, \ldots, d\} - s\) for the rest feature indexes. For convenience, we denote the feature vector \(\xb = (x_1, \cdots , x_s, \cdots, x_D)\) with \((x_s, \xc)\) and the corresponding random variables \(X = (X_1, \cdots , X_s, \cdots, X_D)\) with \((X_s, X_c)\). The training set is \(\mathcal{D} = \{(\xb^i, y^i)\}_{i=1}^N\) sampled i.i.d. from the distribution \(\mathbb{P}_{X,Y}\). Finally, we use \(f^{\mathtt{<method>}}(x_s)\) for the FE of the \(s\)-th feature, with \(\mathtt{<method>}\)
indicating the particular method in use, for example ALE.\footnote{An extensive list of all
  symbols used in the paper is provided at the Appendix.}

\subsection{Feature Effect Methods and ALE}
\label{sec:feat-effect-meth}
The three well-known feature effect methods are: \emph{Partial Dependence
  Plots} (PDP)\citep{friedman2001greedy}, \emph{Marginal Plots}
(MP)\citep{apley2020visualizing} and \emph{Aggregated Local Effects}
(ALE)\citep{apley2020visualizing}
PDP
formulates the FE of the \(s\)-th attribute as an expectation over the
>>>>>>> origin/overleaf-2022-10-11-1029
marginal distribution \(\mathbf{X}_c\), i.e.,
\(f^{\mathtt{PDP}}(x_s) =
\mathbb{E}_{\mathbf{X}_c}[f(x_s,\mathbf{X}_c)]\), whereas MP
formulates it as an expectation over the conditional
\(\mathbf{X}_c|X_s\), i.e.,
\(f^{\mathtt{MP}}(x_s) = \mathbb{E}_{\mathbf{X}_c|x_s}[f(x_s,
\mathbf{X}_c)]\). ALE defines the local effect of the \(s\)-th feature
at instance \(x_s = z\) as
\(f^s(z, \xc) = \frac{\partial f}{\partial x_s} (z, \xc)\). All the
local explanations at \(z\) are, then, weighted by the conditional
distribution \(p(\xc|x_s=z)\) and are averaged, to produce the
averaged effect \(\mu(z)\). ALE is the accumulation of the averaged
local effects:

\begin{equation}
  \label{eq:ALE}
  f^{\mathtt{ALE}}(x_s) = \int_{x_{s,min}}^{x_s} \underbrace{\mathbb{E}_{\Xcb|X_s=z}\left [f^s (z, \Xcb)\right ]}_{\mu(z)} \partial z
\end{equation}

Eq.~(\ref{eq:ALE}) has specific advantages which gain particular value in cases of
correlated input features. In these cases, PDP integrates over
unrealistic instances due to the use of the marginal distribution
\(\mathbf{X}_c \), and MP computes aggregated effects, i.e., imputes
the combined effect of sets of features to a single feature. ALE
manages to resolve both issues, and is therefore the most trustable
method in cases of correlated features. \todo{cite someone}

\subsection{Heterogeneity Of Instance-Level Effects}
\label{sec:quant-heter-effects}

<<<<<<< HEAD
FE methods show what is the average (global) effect on the output, if
the value of a specific feature is increased/decreased. For completing
the explanation about a feature, we need to also know to what extent
the local effects \todo{add what local means here} deviate from the
global explanation, i.e.~to quantify the uncertainty of the average
effect.
\todo{A connector line is needed here on the connection between local-global and uncertainty}
ICE and d-ICE\citep{goldstein2015peeking} provide a set of curves that
are plot on top-of PDP. Both methods produce one curve for each
instance of the training set;
\(f^{\mathtt{ICE}}_i(x_s) = f(x_s, \xc^i)\) for ICE and
\(f^{\mathtt{d-ICE}}_i(x_s) = \frac{\partial f}{\partial x_s} (x_s,
\xc^i)\) for d-ICE. The user then visually observes if the curves are
homogeneous, i.e., all instances have similar values, and to what extent they deviate from the PDP. Some methods
try to automate the aforementioned visual exploration, by grouping
(d-)ICE plots into clusters~\citep{molnar2020model,
  herbinger2022repid, britton2019vine}. Other approaches, like
H-Statistic\citep{friedman2008predictive}, Greenwell's interaction
index\citep{greenwell2018simple} or SHAP interaction
<<<<<<< HEAD
values\citep{lundberg2018consistent} inform indirectly about
heterogeneous effects, quantifying interactions between the input
features. A strong interaction index is an indicator for the existence
of heterogeneous effects.

The aforementioned approaches are under two limitations. They either
do not quantify the uncertainty of the FE directly  \todo{I dont get this argument. What do you mean directly?} or they are based

on PDPs, and, therefore, they are subject to the failure modes of PDPs
in cases of correlated features\citep{baniecki2021fooling}, as we will also show in our experimental analysis (c.f., Section~\ref{sec:simulation-examples-1}). To the best
of our knowledge, no work exist so far that quantifies the
heterogeneous effects based on the formulation of ALE.
\todo{My understanding of the contribution is also the last sentence. None has done it for ALE, but it has been done f for PDPs. For this I think we need some more related owrk on the limiations of PDPs. Also, is this work a straight-forward transfer of the d-ICE etc methods decribed above or transfering these ideas to ALE requires a different approach?}

\subsection{ALE Approximation}
\label{sec:ale-approximation}

<<<<<<< HEAD
In ML scenarios, ALE is estimated from the instances of the training
set.~\citep{apley2020visualizing} proposed dividing the feature domain
in \(K\) equisized bins and estimating the local effects in each bin
by evaluating the black box-function \(f\)at the bin limits: \todo{k or K? I prefer k for the cardinality and it seems that k is used afterwards. }

\begin{equation}
  \label{eq:ALE_accumulated_mean_est}
  \hat{f}^{\mathtt{ALE}}(x_s) = \sum_{k=1}^{k_x} \frac{1}{|\mathcal{S}_k|} \sum_{i:\mathbf{x}^i \in
    \mathcal{S}_k} \left [ f(z_{k}, \xc^i) - f(z_{k-1}, \xc^i)) \right ]
\end{equation}
We denote as \(k_x\) the index of the bin that \(x_s\) belongs to,
i.e. \(k_x: z_{k_x-1} \leq x_s < z_{k_x} \) and \(\mathcal{S}_k\) is
the set of training instances of the \(k\)-th bin, i.e.
\( \mathcal{S}_k = \{ \xb^i : z_{k-1} \leq x^i_s < z_{k} \}
\). 

\citep{gkolemis22} proposed the Differential ALE
(DALE) that computes the local effects on the training instances using
auto-differentiation:
\begin{equation}  \label{eq:DALE_accumulated_mean_est}
  \hat{f}^{\mathtt{DALE}}(x_s) = \Delta x \sum_{k=1}^{k_x} \frac{1}{|\mathcal{S}_k|} \sum_{i:\mathbf{x}^i \in
    \mathcal{S}_k} f^s(\mathbf{x}^i)
\end{equation}
%
Their method has the advantages of remaining on-distribution even when
bins become wider and, most importantly, allows the recomputation of
the accumulated effect with different bin-splitting with near-zero additional
computational cost.

Both approximations, however, require the number of bins \(K\) as
input and perform an equally-sized bin splitting of the attribute
domain. The choice of \(K\) is therefore critical and introduces
significant limitations. Specifically, setting \(K\) to a small value
may hide fine-grain effects due to large bins while setting \(K\) to a
high value leads to poor bin-effect estimations due to few samples per
bin. In general, the user may receive contradictory explanations for
different \(K\) values without a clue for which one to trust.

\section{Uncertainty-Aware ALE (UALE)}
\label{sec:UALE}

UALE extends ALE by quantifying the level of heterogeneous effects
(uncertainty) and by automating the bin-splitting. UALE is introduced in Section~\ref{sec:UALE-definition-1} and redefined using an interval-based formulation in Section~\ref{sec:UALE-definition-2}. UALE approximation is presented in Section~\ref{sec:UALE-approximation} and the optimal bin-splitting in Section~\ref{sec:bin-spliting}.

\subsection{UALE: ALE With Uncertainty Quantification}
\label{sec:UALE-definition-1}

UALE extends ALE definition of Eq.~(\ref{eq:ALE}) with a component for
quantifying the uncertainty. We denote as \(\mathcal{H}(z)\) the
\emph{uncertainty} of the local effects at a specific point \(x_s=z\) and we
quantify it with the standard deviation of the local explanations
\(\mathcal{H}(z) = \sigma(z)\), where:

\begin{equation}
  \label{eq:ALE_var}
  \sigma^2(z) = \mathbb{E}_{\Xcb|z}\left [ \left (\dfdx (z, X_c) - \mu(z) \right )^2 \right ] 
\end{equation}
\noindent
The uncertainty emerges from the natural characteristics of the
experiment\todo{should we say of the population?} , i.e.,~the feature correlations existent in the data
generating distribution and the implicit interactions of\todo{of or in?} the black-box
function. We also define the \emph{accumulated uncertainty} at \(x_s\), as
the accumulation of the standard deviation of the local effects along
the axis:
\begin{equation}
  \label{eq:ALE_acc_unc}
  f^{\mathtt{ALE}}_{\sigma}(x_s) = \int_{x_{s, min}}^{x_s} \sigma(z) \partial z
\end{equation}
\noindent
UALE formulates the effect at a specific point \(x_s\) with a tuple
that consists of the average effect and the uncertainty
\((\mu(z), \sigma(z))\) and visualizes it with a continuous curve \todo{we could point to some experiment?} and
a confidence region:\\
\begin{equation}
  \label{eq:UALE}
  f^{\mathtt{UALE}}(x_s) := f^{\mathtt{ALE}}_{\mu}(x_s) \pm
f^{\mathtt{ALE}}_{\sigma}(x_s)
  \end{equation}
  
%\(f^{\mathtt{UALE}}(x_s) := f^{\mathtt{ALE}}_{\mu}(x_s) \pm
%f^{\mathtt{ALE}}_{\sigma}(x_s)\)

\subsection{Interval-based Formulation and Approximation}
\label{sec:UALE-definition-2}

In real ML scenarios, all estimations are based on the instances of
the training set. Therefore, we cannot estimate \(\mu(z), \sigma(z)\)
at the granularity of a point, because the probability of observing a
sample inside the region \([x_s - h, x_s + h]\) tends to zero, when
\(h \to 0\). Therefore, we abstract at region/interval level \([z_1, z_2)\) and we define the \emph{regional-effect} and the
\emph{regional-uncertainty} \(\mathcal{H}(z_1, z_2) = \sigma(z_1, z_2)\), as
the summary statistics that characterize the effect and the
uncertainty inside the interval \([z_1, z_2)\) as follows:
\begin{equation}
  \label{eq:mu_bin}
    \mu(z_1, z_2) = \mathbb{E}_{z \sim \mathcal{U}(z_1,z_2)} [\mu(z)]
    = \frac{\int_{z_1}^{z_2} \mu(z) \partial z}{z_2 - z_1}
\end{equation}

\begin{equation}
  \label{eq:var_bin}
  \sigma^2(z_1, z_2) = \mathbb{E}_{z \sim \mathcal{U}(z_1,z_2)} [\sigma^2(z)] =  \frac{\int_{z_1}^{z_2} \sigma^2(z)  \partial z}{z_2 - z_1}
\end{equation}

%
Intuitively, the regional-effect and the regional-uncertainty quantify
the expected average effect and the expected uncertainty if we
randomly draw a point \(z^*\) given a uniform distribution
\(z^* \sim \mathcal{U}(z_1, z_2)\). If we also define as
\(\mathcal{Z}\) the sequence of \(K+1\) points that partition the
domain of the \(s\)-th feature into \(K\) variable-size intervals,
i.e.  \(\mathcal{Z} = \{z_0, \ldots, z_K\}\), we can redefine UALE
using an interval-based formulation as follows:
\begin{equation}
\label{eq:UALE_approx}
\tilde{f}^{\mathtt{UALE}}_{\mathcal{Z}}(x_s):= \tilde{f}^{\mathtt{ALE}}_{\mathcal{Z}, \mu}(x_s)
\pm \tilde{f}^{\mathtt{ALE}}_{\mathcal{Z}, \sigma}(x_s)   
\end{equation}
where:

\begin{equation}
  \label{eq:ALE_2}
\tilde{f}^{\mathtt{ALE}}_{\mathcal{Z}, \mu}(x_s) = \sum_{k=1}^{k_x} \mu(z_{k-1}, z_k) (z_k - z_{k-1})
\end{equation}

\begin{equation}
  \label{eq:ALE_accumulated_var}
  \tilde{f}^{\mathtt{ALE}}_{\mathcal{Z}, \sigma}(x_s) =  \sum_{k=1}^{k_x} \sigma(z_{k-1}, z_k) (z_k - z_{k-1})
\end{equation}
%

\subsubsection{Interval-Based Approximation}
\label{sec:UALE-approximation}

For approximating the mean effect (Eq.~\eqref{eq:mu_bin}) and the
uncertainty (Eq.~\eqref{eq:var_bin}) we use the set \(\mathcal{S}_k\)
of dataset instances with the \(s\)-th feature lying inside the
interval
\( \mathcal{S}_k= \{ \mathbf{x}^i : z_{k-1} \leq x^i_s < z_k \} \):

\begin{equation}
  \label{eq:2}
  \hat{\mu}(z_{k-1}, z_k) = \frac{1}{|\mathcal{S}_k|}
  \sum_{i:\mathbf{x}^i \in \mathcal{S}_k} \left [ \dfdx(\mathbf{x}^i)
  \right ]
\end{equation}
%
\begin{equation}
  \label{eq:3}
  \hat{\sigma}^2(z_{k-1}, z_k) = \frac{1}{|\mathcal{S}_k|}
\sum_{i:\mathbf{x}^i \in \mathcal{S}_k} \left ( \dfdx(\mathbf{x}^i) -
  \hat{\mu}(z_1, z_2) \right )^2
\end{equation}

Eq.\eqref{eq:2} is an unbiased estimator of Eq.~(\ref{eq:mu_bin})
under the assumption that the points are uniformly distributed inside
the intervals. The same does not hold in the general case for
Eq.\eqref{eq:3}. In the general case, \(\hat{\sigma}^2(z_{k-1}, z_k)\)
is an unbiased estimator of the \textit{observable} variance
\(\sigma^2_{obs}(z_1, z_2) = \frac{\int_{z_1}^{z_2}
    \mathbb{E}_{\xc|x_s=z} \left [ (f^s(z, \xc) - \mu(z_1, z_2) )^2
    \right] \partial z}{z_2 - z_1}\), which as we prove in Theorem 1
is an overestimation of \(\sigma^2(z_1, z_2)\).

\paragraph{Theorem 1.}
\label{sec:theorem-1}

\textit{If we define the residual \(\rho(z)\) as the difference
  between the expected effect at \(x_s\) and the regional effect, i.e
  \(\rho(z) = \mu(z) - \mu(z_1, z_2)\), then the regional variance
  \(\sigma^2(z_1, z_2)\) equals to:}

\begin{equation}
    \label{eq:bin-uncertainty-proof}
 \sigma_{obs}^2(z_1, z_2) = \sigma^2(z_1, z_2) + \frac{\int_{z_1}^{z_2}\rho^2(z) \partial z}{z_2 - z_1}
\end{equation}
\noindent
Theorem 1 reveals an important connection between the ground-truth and
the observable uncertainty inside a region. For convenience, we denote
as
\(\mathcal{E}^2(z_1, z_2) = \frac{\int_{z_1}^{z_2}\rho^2(z) \partial
  z}{z_2 - z_1}\) the error term that models the expected square
residual inside the interval and as
\(\mathcal{H}_{obs}(z_1, z_2) := \sigma_{obs}(z_1, z_2)\) the
observable uncertainty. It holds that the observable uncertainty is an
overestimation of the correct uncertainty

\begin{equation}
  \label{eq:uncertainty-bin}
  \mathcal{H}^2_{obs}(z_1, z_2) = \mathcal{H}^2(z_1, z_2) + \mathcal{E}^2(z_1, z_2)
\end{equation}
%
and, therefore, the estimation is unbiased only in case
\(\mathcal{E}^2(z_1, z_2) = 0\).

\subsection{Bin-Splitting: Finding Regions With Homogeneous Effects}
\label{sec:bin-spliting}
\todo{I think we need to address here also the parameter K, how many bins is an input parameter. Maybe we can say the user decides based on their needs and how many bins they can manage}
In this section we formulate bin-splitting as an unsupervised
clustering problem, searching for a solution that compromises two
contradictory objectives. On the one hand, we want to minimize the
error term \(\mathcal{E}\) for approaching an unbiased estimation and
on the other hand, we want wide-enough bins for including a fair\todo{sufficient/ representative?}
population of samples for a robust estimation of
\(\hat{\mu}(z_1, z_2), \hat{\sigma}(z_1, z_2)\). 
We formulate this as an optimization problem:

\begin{equation}
  \label{eq:opt}
\begin{aligned}
  \min_{ \mathcal{Z} = \{z_0, \ldots, z_K\}} \quad & \mathcal{L} = \sum_{k=1}^K \tau_k \hat{\sigma}^2(z_{k-1}, z_k) \Delta z_k \\
  \textrm{where} \quad & \Delta z_k = z_k - z_{k-1} \\
  & \tau_k = 1 - \alpha \frac{|S_k|}{N} \\
  \textrm{s.t.} \quad & |\mathcal{S}_k| \geq N_{\mathtt{PPB}}\\
                                     & z_0 = x_{s,min}\\
                                     & z_K = x_{s, max}
\end{aligned}
\end{equation}
%
We search for the sequence of intervals \(z_0, \ldots, z_K\) that
minimizes the sum of the bin costs. The cost of each bin is the
approximated variance \(\hat{\sigma}^2_k\) scaled by the bin length
\(\Delta z_k\) and discounted by the term \(\tau_k\). The term
\(\tau_K\) favors the selection of a bigger bin in case it has similar
variance with the aggregate variance of many smaller ones. The
constraint of at least \(N_{\mathtt{PPB}}\) points per bin sets the
lowest-limit for a \textit{robust} estimation. The user can choose to
what extent they favor the creation of wide bins through the discount
parameter by setting the parameter \(\alpha\) and where they set the
minimum for robust approximation with the parameter
\(N_{\mathtt{PPB}}\). 
%For providing a rough idea,
In our experiments
we chose \(\alpha = 0.2\) which means that the discount ranges between
\([0\%, 20\%]\) and \(N_{\mathtt{PPB}} = \frac{N}{20}\). It is also
important to clarify that by minimizing
\(\hat{\sigma}^2_k \approx \mathcal{H}_{obs}^2\), we actually minimize
the squared error term \(\mathcal{E}^2\), since the term
\(\mathcal{H}^2_{true}\) is independent of the bin splitting.

\subsubsection{Solve Bin-Splitting with Dynamic Programming}
\label{sec:dynamic-programing}

\todo{Review all the description}
To solve the bin-splitting problem (c.f., Equation~\ref{eq:opt}), we use dynamic programming. 
We set an upper limit on the \(K_{max}\) on the number of bins to achieve a computationally-grounded solution, based on this the solution space is discretized. The width of the bins can take discrete values that are
multiple of the minimum step
\(u = \frac{x_{s, max} - x_{s, min}}{K_{max}}\). For defining the
solution, we use two indexes: index
\(i \in \{0, \ldots, K_{max}\}\) denotes the point \((z_i)\) whereas 
index \(j \in \{0, \ldots, K_{max}\} \) denotes the position of the
\(j\)-th multiple of the minimum step, i.e., 
\(x_j = x_{s,min} + j \cdot u\). The recursive cost function
\(T(i,j)\) is the cost of setting \(z_i=x_j\):
\begin{equation}
  \label{eq:recursive_cost}
  \mathcal{T}(i,j) = \mathrm{min}_{l \in \{0, \ldots, K_{max}\}} \left [ \mathcal{T}(i-1, l) + \mathcal{B}(x_l, x_j) \right ]
\end{equation}
%
where \(\mathcal{T}(0,j)\) equals zero if \(j=0\) and \(\infty\) in
any other case. \(\mathcal{B}(x_l, x_j)\) denotes the cost of creating a bin
with limits \([x_l, x_j)]\):

\begin{equation}
  \label{eq:cost_step}
  \mathcal{B}(x_l, x_j) = \begin{cases}
                            \infty, & \text{if $x_j > x_l$ or \(|\mathcal{S}_{(x_j, x_l)}| < N\)}\\
                            0, & \text{if $x_j = x_l$}\\
                            \hat{\sigma}^2(x_j, x_l), &\text{if $x_j \leq x_l$}
  \end{cases}
\end{equation}

The optimal solution is given by solving
\(\mathcal{L} = \mathcal{T}(K_{max}, K_{max})\) and keeping track of the sequence of
steps. 

\todo{Discuss more aspects (e.g. Computational complexity); is the solution optimal?}

\section{Experimental evaluation: Synthetic data}
\label{sec:simulation-examples}
It is a common practice in XAI research, e.g.,~\citep{aas2021explaining, herbinger2022repid}, to use synthetic data for the evaluation, since the
data-generating distribution \(p(\mathbf{X})\) and the predictive
function \(f(\cdot)\) are known, and, therefore, the ground-truth is
accessible. We follow this common approach and we evaluate two different aspects: i) how UALE compares to PDP-ICE w.r.t. average effect and uncertainty estimates (Section~\ref{sec:simulation-examples-1}) and, ii) how automatic-bin splitting compares to vanilla fixed-size bin splitting w.r.t. the quality of approximation (Section~\ref{sec:simulation-examples-2}).

\subsection{UALE vs PDP-ICE}
\label{sec:simulation-examples-1}

In this simulation, we compare UALE against PDP-ICE. The aim of this
example is to highlight that when features are correlated, PDP and ICE
fail in quantifying the average effect and the heterogeneity, even
when the black-box function is relatatively simple.

We use the following distribution:
\(p(\mathbf{x}) = p(x_3)p(x_2|x_1)p(x_1)\) where
\(x_1 \sim \mathcal{U}(0,1)\), \(x_2 = x_1\) and
\(x_3 \sim \mathcal{N}(0, \sigma_3^2)\) and the following piecewise
linear function:

\begin{equation}
  \label{eq:synth-ex-1-function}
  f(\mathbf{x}) = \begin{cases}
                    f_{\mathtt{lin}} + \alpha f_{\mathtt{int}} & \text{if $f_{\mathtt{lin}} < 0.5$ }\\
                    0.5 - f_{\mathtt{lin}} + \alpha f_{\mathtt{int}} & \text{if $0.5 \leq f_{\mathtt{lin}} < 1$}\\
                    \alpha f_{\mathtt{int}} &\text{otherwise}
                  \end{cases}
\end{equation}
%
where \(f_{\mathtt{lin}}(\mathbf{x}) = a_1 x_1 + a_2 x_2\) and
\(f_{\mathtt{int}}(\mathbf{x}) = x_1x_3\). The linear term
\(f_{\mathtt{lin}}\) includes the two correlated features and the
interaction term \(f_{\mathtt{int}}\) the two non-correlated.  We
evaluate the effect computed by UALE and PDP-ICE in three cases; (a)
without interaction (\(\alpha=0\)) and equal weights (\(a_1=a_2\)),
(b) without interaction (\(\alpha=0\)) and different weigths
(\( a_1 \neq a_2 \)) and (c) with interaction (\(\alpha \neq 0\)) with
equal weights (\(a_1=a_2\)). In all cases, we provide the ground-truth
average effect and uncertainty computed with analytical derivations
(proofs at the Appendix) and we compare it against the approximations
provided by each method, after generating \(N=500\) samples.

\paragraph{No Interaction, Equal weights.}

In this set-up, \(\alpha=0\) (no interaction) and the \(a_1=a_2=1\)
(equal weights). The ground truth effect \(f_\mu^{\mathtt{GT}}(x_1)\)
is: \(x_1\) in \([0, 0.25]\), \(-x_1\) in \([0.25, 0.5]\) and \(0\) in
\([0.5, 1]\). The uncertainty is
\(f^{\mathcal{GT}}_{\sigma^2}(x_1) = 0\) in \([0,1]\) because \(x_1\)
does not interact with ohter variables. In
Figure~\ref{fig:synth-ex-1-case-1}, we observe that PDP effect is
wrong and ICE plots imply the existence of heterogeneous effects. In
contrast, UALE models correctly both the average effect and the zero
uncertainty and approximates it accurately finding a zero aggregate
residual error partitioning of \(x_1\) domain.

\begin{figure}[h]
  \centering
  \includegraphics[width=.23\textwidth]{example_1/dale_feat_0.pdf}
  \includegraphics[width=.23\textwidth]{example_1/pdp_ice_feat_0.pdf}
  \caption{No interaction, Equal weights: Feature effect for \(x_1\)
    using UALE (Left) and PDP-ICE (Right).}
  \label{fig:synth-ex-1-case-1}
\end{figure}

\paragraph{No Interaction, Non-Equal Weights.}

In this set-up, \(\alpha=0\) (no interaction), \(a_1 = 2\) and
\(a_2= 0.5\) (non-equal weights). The non-equal weights have
implications at both the gradient and the interval of the piece-wise
linear regions, i.e., \(f_\mu^{\mathtt{GT}}(x_1)\) is: \(2x_1\) in
\([0, 0.2]\), \(-2x_1\) in \([0.2, 0.4]\) and \(0\) in \([0.4,
1]\). As before, the ground-truth uncertainty is
\(f^{\mathcal{GT}}_{\sigma^2}(x_1) = 0\) because \(x_1\) does not
interact with other features.  In Figure~\ref{fig:ex-synth-1-2}, we
observe that PDP estimation is completely opposite to the ground-truth
effect, i.e.~negative in the region \([0, 0.2)\), positive in
\([0.2, 0.4)\), and the ICE erroneously implies the existence of
heterogeneous effects. As before, ALE quantifies correctly the ground
truth effect, the zero-uncertainty and partioning correctly the
\(x_1\) domain.

\begin{figure}[h]
  \centering
  \includegraphics[width=.23\textwidth]{example_2/dale_feat_0.pdf}
  \includegraphics[width=.23\textwidth]{example_2/pdp_ice_feat_0.pdf}
  \caption{No interaction, Different weights: Feature effect for \(x_1\)
    using UALE (Left) and PDP-ICE (Right).}
  \label{fig:ex-synth-1-2}
\end{figure}

\paragraph{Uncertainty, Equal weights.}

In this set-up, we activate the interaction term, i.e.  \(a=1\),
keeping the weights equal \(a_1=a_2=1\). The interaction term provokes
heterogeneous effects for features \(x_1\) and \(x_3\), because the
local effects at \(x_1\) depend on the unknown value of \(X_3\) and
vice-versa. The effect of \(x_2\) has zero-uncertainty since it does
not appear in any interaction term.  As we observe in
Figure~\ref{fig:ex-synth-1-3}, UALE correcly models the average effect
and the uncertainty in all cases.  On the other hand, PDP-ICE
quantifies correctly the average effect and the uncertainty only in
the case of feature \(x_3\), because \(X_3\) is independent from other
features. For the correlated features (\(x_1, x_2\)) both the average
effect (PDP) and the uncertainty (ICE) are wrong.

\begin{figure}[h]
  \centering
  \includegraphics[width=.23\textwidth]{example_3/dale_feat_0.pdf}
  \includegraphics[width=.23\textwidth]{example_3/pdp_ice_feat_0.pdf}\\
  \includegraphics[width=.23\textwidth]{example_3/dale_feat_1.pdf}
  \includegraphics[width=.23\textwidth]{example_3/pdp_ice_feat_1.pdf}\\
  \includegraphics[width=.23\textwidth]{example_3/dale_feat_2.pdf}
  \includegraphics[width=.23\textwidth]{example_3/pdp_ice_feat_2.pdf}\\
  \caption{With interaction, equal weights: From top to bottom,
    feature effect for features \(\{x_1, x_2, x_3\}\) using UALE (left
    column) and PDP-ICE (right column).}
  \label{fig:ex-synth-1-3}
\end{figure}

\paragraph{Discussion.}

Despite the model's simplicity, PDP-ICE fail in modeling both the
average effect and the uncertainty. The error is due to the method's
formulation, i.e.~the use of the marginal distribution that ignores
correlations between features, and not due to poor approximation
because of limited samples. In contrast, UALE models both correctly
and estimates them accurately extracting the regions with constant
effect.  The examples above do not cover the case of an interaction
term between correlated features, for example a term \(x_1x_2\),
because there is an open debate about the ground-truth effect in this
case~\citep{Gromping2020MAEP}.

\subsection{Bin-Splitting}
\label{sec:simulation-examples-2}

In this simulation, we illustrate the advantages of automatic
bin-splitting against the fixed-size alternative. For this reason, we
generate a very big dataset applying dense sampling (\(N=10^6\)) and
we treat the estimation with dense fixed-size bins (\(K=10^3\)) as the
ground-truth UALE. Afterwards, we generate less samples (\(N=500\))
and we compare the fixed-size estimation (for many different \(K\))
against UALE automatic bin-splitting. In all set-ups, we sample from
\(p(\mathbf{x}) = p(x_2|x_1)p(x_1)\) where
\(x_1 \sim \mathcal{U}(0,1)\) and
\(x_2 \sim \mathcal{N}(x_1, \sigma_2^2=0.5)\). We denote as
\(\mathcal{Z^{\mathtt{DP}}} = \{z^{\mathtt{DP}}_{k-1}, \cdots,
z^{\mathtt{DP}}_{K}\}\) the sequence obtained by automatic
bin-splitting and with \(\mathcal{Z^{\mathtt{K}}}\) the fixed-size
splitting with \(K\) bins. The evaluation metrics we report are the
average number of \(t = 30\) independent runs, using each time
\(N=500\) different samples.

\paragraph{Metrics}

The evaluation is done with regard to two metrics counting the mean
error per bin, where the bin error is definde as the absolute
difference of the approximation from the ground-truth. The first
metric, Eq.~\eqref{eq:eval_met_1}, counts the mean bin error wrt
average effect and the second, Eq.~\eqref{eq:eval_met_2} the mean bin
error wrt uncertainty:

\begin{equation}
  \label{eq:eval_met_1}
  \mathcal{L}_{\mathtt{DP|K}}^{\mu} = \frac{1}{|\mathcal{Z}^{\mathtt{DP|K}}|} \sum_{k \in
  \mathcal{Z}^{\mathtt{DP|K}}} | \mu(z_k- z_{k-1}) - \hat{\mu}(z_k-
  z_{k-1}) |
\end{equation}


\begin{equation}
  \label{eq:eval_met_2}
  \mathcal{L}_{\mathtt{DP|K}}^{\sigma} = \frac{1}{|\mathcal{Z}^{\mathtt{DP|K}}|} \sum_{k \in
    \mathcal{Z}^{\mathtt{DP|K}}} | \sigma(z_k- z_{k-1}) - \hat{\sigma}(z_k-
  z_{k-1}) |
\end{equation}

The ground truth \(\mu_k\) (\(\sigma_k\)) is the mean value of the
bin-effects (bin-uncertainties) of the bins inside the interval
defined by the wide bin. For better interpretation of the uncertainty
error, we also provide the mean (per bin) error term
\(\mathcal{L}^{\rho}_{\mathtt{DP|K}} =
\frac{1}{|\mathcal{Z}^{\mathtt{DP|K}}|} \sum_{k \in
  \mathcal{Z}^{\mathtt{DP|K}}} \mathcal{E}(z_{k-1}, z_k) \).

\paragraph{Piecewise-Linear Function.}

In this set-up, \(f(\mathbf{x}) = a_1x_1 + x_1x_2\) is a
piecewise-linear function with \(5\) different-width regions, i.e.,
\(a_1\) equals to \(=\{2, -2, 5, -10, 0.5\}\) in the intervals defined
by the sequence \(\{0, 0.2, 0.4, 0.45, 0.5, 1\}\). As we observe, in
Figure~\ref{fig:ex-synth-2-1}, UALE's approximation is better than all
fixed-size alternatives, in both mean effect (\(\mathcal{L}^{\mu}\))
and uncertainty (\(\mathcal{L}^{\sigma}\)). For understanding the
importance of variable-size splitting, we analyze the fixed-bin
error. In the top-right of Figure~\ref{fig:ex-synth-2-1}, we observe
the Law of Large Numbers (LLN); \(\mathcal{L}^{\mu}_{\mathtt{K}}\) and
\(K\) have positive correlation because as \(\Delta x\) becomes
smaller less points contributing to each estimation. The
interpretation of \(\mathcal{L}^{\sigma}\) is more complex. Given that
the smallest piecewise-linear region is \(\Delta x = 0.05 \) any
\(K\) that is not a multiple of \(\frac{1}{\Delta x} = 20\), adds
nuisance uncertainty \(\mathcal{L}^{\rho}_{\mathtt{K}} > 0\) leading
to a biased uncertainty approximation. For these \(K\), uncertainy
error decreases for bigger \(K\) because the length of the bins that
lie in the limit between two piecewise linear regions becomes
smaller. For \(K\) that are multiple of \(20\), i.e.
\(K=\{20, 40, 60, 80, 100\}\), the uncertainty approximation is
unbiased \(\mathcal{L}^{\rho}_{\mathtt{K}} \approx 0\). Here, as in
\(\mathcal{L}^{\mu}\), uncertainty estimation is worse as \(K\) grows
larger (LLN). As we observe in the top left of
Figure~\ref{fig:ex-synth-2-1}, automatic bin-splitting separates the
fine-grain bins, e.g.~regions \([0.4, 0.45]\), \([0.45, 0.5]\), and
unites constant-effect regions into a single bin, e.g.~region
\([0.5, 1]\).

\begin{figure}[h]
  \centering
  \includegraphics[width=.23\textwidth]{example_bin_splitting_1/fig_1.pdf}
  \includegraphics[width=.23\textwidth]{example_bin_splitting_1/fig_2.pdf}\\
  \includegraphics[width=.23\textwidth]{example_bin_splitting_1/fig_3.pdf}
  \includegraphics[width=.23\textwidth]{example_bin_splitting_1/fig_4.pdf}
  \caption{Figure 1}
  \label{fig:ex-synth-2-1}
\end{figure}


\paragraph{Non-Linear Function.}

In this set-up, \(f(\mathbf{x}) = 4x_1^2 + x_2^2 + x_1x_2\), so the
effect is non-linear in \(0 \leq x_1 \leq 1\). In this set-up, wide
bins increase \(\mathcal{L}^{\rho^2}\) making the uncertainty
approximation more biased and narrow bins lead to a worse
approximation. In Figure~\ref{fig:ex-synth-2-2}, we observe that
automatic bin splitting manages to compromise the
conflicting objectives, \todo{Explain better}

\begin{figure}[h]
  \centering
  \includegraphics[width=.23\textwidth]{example_bin_splitting_2/fig_1.pdf}
  \includegraphics[width=.23\textwidth]{example_bin_splitting_2/fig_2.pdf}\\
  \includegraphics[width=.23\textwidth]{example_bin_splitting_2/fig_3.pdf}
  \includegraphics[width=.23\textwidth]{example_bin_splitting_2/fig_4.pdf}
  \caption{Figure 1}
  \label{fig:ex-synth-2-2}
\end{figure}

\section{Experimental evaluation: Real data}

Here, we aim at demonstrating the usefuleness of uncertainty
quantification and the advantages of automatic bin-splitting, on the
real-world California Housing dataset~\citep{pace1997sparse}.

\paragraph{ML setup}

The California Housing is a largely-studied dataset with approximately
\(20000\) training instances, making it appropriate for robust
Monte-Carlo approximations. The dataset contains \(D=8\) numerical
features with characteristics about the building blocks of California,
e.g. latitude, longitude, population of the block or median age of
houses in the block. The target variable is the median value of the
houses inside the block in dollars that ranges between
\([15, 500] \cdots 10^3\), with a mean value of
\(\mu_Y \approx 201 \cdot 10^3 \) and a standard deviation of
\(\sigma_Y \approx 110 \cdot 10^3\).

We exclude instances with missing and outlier values. As outlier we
define the feature values which ar over three standard deviations away
from the mean feature value. We also normalize all features to
zero-mean and unit standard deviation. We split the dataset into
\(N_{tr} = 15639\) training and \(N_{test} = 3910\) test examples
(80/20 split) and we fit a Neural Network with 3 hidden layers of 256,
128 and 36 units respectively. After 15 epochs using the Adam
optimizer with learning rate \(\eta = 0.02\), the model achieves a
normalized mean squared error (R-Squared) of \(0.25\) (0.75), which
corresponds to a MAE of \(37 \cdot 10^3\) dollars.

Below, we illustrate the feature effect for three features: latitude
\(x_2\), population \(x_6\) and median income \(x_8\). The particular
features cover the main FE cases, e.g.~positive/negative trend and
linear/non-linear curve, and they are appropriate for illustration
purposes. The complete results for all features, along with in-depth
information about the preprocessing, training and evaluation parts are
provided in the Appendix.

\paragraph{Uncertainty Quantification}

In real-world datasets, it is infeasible to obtain the ground truth FE
for seamlessly evaluating the competitive methods. We selected the
particular experiment, because, in broad terms, UALE and PDP-ICE plots
aggree in the estimation of the average effect and
uncertainty. Therefore, we can focus on juding the quality of the
information provided by the two methods.

In Figure\ref{fig:ex-real-1}, we observe, from top to bottom, the
effects for the latitude, population and the median income. The effect
of UALE and PDP-ICE are similar for the population and the median
income. The population has a negative impact that progressively
decreases: from 400 to 1500 people the house value decreases with a
rate of \(-150 (\pm 140)\) dollars per added person, a rate that
decreases from \(-80 (\pm 80)\) to \(-60 (\pm 60)\) dollars per added
person as we move from 1500 to 2800 people. The level of uncertainty
indicates significant variance in absolute value of the rate, but in
the grant majority of instances the rate is negative. With the same
inspection, we observe that the median annual income has a positive
impact on the value (all numbers are thousands of dollars):
\(20\pm 15\) per \(10\) of added median income for incomes in
\([8, 15]\), \(32 \pm 20\) per \(10\) added income in \([15, 60]\) and
\(40 \pm 15\) per \(10\) added income in \([60, 70]\). The uncertainty
indicates that there are less heterogeneous effects about the median
income compared to the number of people. In both cases, we can end-up
to the same conclusion by inspecting the PDP-ICE plots. For the
latitude, there is a small difference in the explanations for the
region \([32, 35]\), where UALE estimates a less negative slope with
less uncertainty than PDP, while the explanations are similar for the
range \([35,39]\), where both methods reveal an increase in the
uncertainty around the feature value \(37.5\).

In general, we observe that UALE complements ALE in the quantification
of the heterogeneous effects, similarly to as ICE complements PDP. The
automatic extraction of constant-effect and constant-uncertainty
regions provided by UALE is helpful for an easier interpretation. On
the other hand, ICE plots sometimes are more descritive on locating
the type of heterogeneity, whereas UALE quantifies only the level (not
the type) of heterogeneity.

\begin{figure}[h]
  \centering
  \includegraphics[width=.23\textwidth]{real_dataset_3/feature_1_ale_auto.pdf}
  \includegraphics[width=.23\textwidth]{real_dataset_3/feature_1_pdp_ice.pdf}\\
  \includegraphics[width=.23\textwidth]{real_dataset_3/feature_5_ale_auto.pdf}
  \includegraphics[width=.23\textwidth]{real_dataset_3/feature_5_pdp_ice.pdf}\\
  \includegraphics[width=.23\textwidth]{real_dataset_3/feature_7_ale_auto.pdf}
  \includegraphics[width=.23\textwidth]{real_dataset_3/feature_7_pdp_ice.pdf}\\
  \caption{Figure 1}
  \label{fig:ex-real-1}
\end{figure}

\paragraph{Bin Splitting}

In this part, we evaluate the robustness of the approximation using
automatic bin-splitting. Following the evaluation framework of
Section~\ref{sec:simulation-examples-2}, we treat as ground-truth the
effects computed on the full training-set \(N=20000\) with dense
fixed-size bin-splitting (\(K=80\)). Given the big number of samples,
we make the hypothesis that the approximation with dense binning is
close to the ground truth. Afterwards, we randomly select less samples
\(N=1000\) and we compare UALE approximation with all possible
fixed-size alternatives, repeating this process for 30 times for
robust results. In Figure~\ref{fig:ex-real-2}, we illustrate the mean
values for \(\mathcal{L}^{\mu}, \mathcal{L}^{\sigma}\) of the 30
repetitions. We observe that automatic bin-spliting provides (close to
the) best approximation in the three features. In the Appendix, we
provide the same evaluation for all features.

\begin{figure}[h]
  \centering
  \includegraphics[width=.23\textwidth]{real_dataset_3/compare_mu_err_feature_1.pdf}
  \includegraphics[width=.23\textwidth]{real_dataset_3/compare_var_err_feature_1.pdf}\\
  \includegraphics[width=.23\textwidth]{real_dataset_3/compare_mu_err_feature_5.pdf}
  \includegraphics[width=.23\textwidth]{real_dataset_3/compare_var_err_feature_5.pdf}\\
  \includegraphics[width=.23\textwidth]{real_dataset_3/compare_mu_err_feature_7.pdf}
  \includegraphics[width=.23\textwidth]{real_dataset_3/compare_var_err_feature_7.pdf}\\
  \caption{Figure 1}
  \label{fig:ex-real-2}
\end{figure}

\section{DISCUSSION}

\todo{Add Conclusion, limitations, and future work}

\subsubsection*{Acknowledgments}
All acknowledgments go at the end of the paper, including thanks to reviewers who gave useful comments, to colleagues who contributed to the ideas, and to funding agencies and corporate sponsors that provided financial support. 
To preserve the anonymity, please include acknowledgments \emph{only} in the camera-ready papers.

\bibliography{biblio}

\end{document}
